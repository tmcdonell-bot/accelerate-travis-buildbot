-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | Accelerate backend generating LLVM
--   
--   This library implements a backend for the <i>Accelerate</i> language
--   which generates LLVM-IR targeting multicore CPUs. For further
--   information, refer to the main <i>Accelerate</i> package:
--   <a>http://hackage.haskell.org/package/accelerate</a>
@package accelerate-llvm-native
@version 1.0.0.0


module Data.Array.Accelerate.LLVM.Native.Foreign
data ForeignAcc f
[ForeignAcc] :: String -> (a -> LLVM Native b) -> ForeignAcc (a -> b)
data ForeignExp f
[ForeignExp] :: String -> IRFun1 Native () (x -> y) -> ForeignExp (x -> y)

-- | The LLVM monad, for executing array computations. This consists of a
--   stack for the LLVM execution context as well as the per-execution
--   target specific state <tt>target</tt>.
data LLVM target a :: * -> * -> *

-- | Native machine code JIT execution target
data Native

-- | Lift a computation from the <a>IO</a> monad.
liftIO :: MonadIO m => forall a. IO a -> m a

-- | Copy an array into a newly allocated array. This uses <a>memcpy</a>.
cloneArray :: (Shape sh, Elt e) => Array sh e -> LLVM Native (Array sh e)
instance Data.Array.Accelerate.LLVM.Foreign.Foreign Data.Array.Accelerate.LLVM.Native.Target.Native
instance Data.Array.Accelerate.Array.Sugar.Foreign Data.Array.Accelerate.LLVM.Native.Foreign.ForeignAcc
instance Data.Array.Accelerate.Array.Sugar.Foreign Data.Array.Accelerate.LLVM.Native.Foreign.ForeignExp


-- | This module implements a backend for the <i>Accelerate</i> language
--   targeting multicore CPUs. Expressions are on-line translated into LLVM
--   code, which is just-in-time executed in parallel over the available
--   CPUs. Functions are automatically parallel, provided you specify '+RTS
--   -Nwhatever' on the command line when running the program.
module Data.Array.Accelerate.LLVM.Native

-- | Accelerate is an <i>embedded language</i> that distinguishes between
--   vanilla arrays (e.g. in Haskell memory on the CPU) and embedded arrays
--   (e.g. in device memory on a GPU), as well as the computations on both
--   of these. Since Accelerate is an embedded language, programs written
--   in Accelerate are not compiled by the Haskell compiler (GHC). Rather,
--   each Accelerate backend is a <i>runtime compiler</i> which generates
--   and executes parallel SIMD code of the target language at application
--   <i>runtime</i>.
--   
--   The type constructor <a>Acc</a> represents embedded collective array
--   operations. A term of type <tt>Acc a</tt> is an Accelerate program
--   which, once executed, will produce a value of type <tt>a</tt> (an
--   <a>Array</a> or a tuple of <a>Arrays</a>). Collective operations of
--   type <tt>Acc a</tt> comprise many <i>scalar expressions</i>, wrapped
--   in type constructor <a>Exp</a>, which will be executed in parallel.
--   Although collective operations comprise many scalar operations
--   executed in parallel, scalar operations <i>cannot</i> initiate new
--   collective operations: this stratification between scalar operations
--   in <a>Exp</a> and array operations in <a>Acc</a> helps statically
--   exclude <i>nested data parallelism</i>, which is difficult to execute
--   efficiently on constrained hardware such as GPUs.
--   
--   For example, to compute a vector dot product we could write:
--   
--   <pre>
--   dotp :: Num a =&gt; Vector a -&gt; Vector a -&gt; Acc (Scalar a)
--   dotp xs ys =
--     let
--         xs' = use xs
--         ys' = use ys
--     in
--     fold (+) 0 ( zipWith (*) xs' ys' )
--   </pre>
--   
--   The function <tt>dotp</tt> consumes two one-dimensional arrays
--   (<a>Vector</a>s) of values, and produces a single (<a>Scalar</a>)
--   result as output. As the return type is wrapped in the type
--   <a>Acc</a>, we see that it is an embedded Accelerate computation - it
--   will be evaluated in the <i>object</i> language of dynamically
--   generated parallel code, rather than the <i>meta</i> language of
--   vanilla Haskell.
--   
--   As the arguments to <tt>dotp</tt> are plain Haskell arrays, to make
--   these available to Accelerate computations they must be embedded with
--   the <a>use</a> function.
--   
--   An Accelerate backend is used to evaluate the embedded computation and
--   return the result back to vanilla Haskell. Calling the <tt>run</tt>
--   function of a backend will generate code for the target architecture,
--   compile, and execute it. For example, the following backends are
--   available:
--   
--   <ul>
--   <li><a>accelerate-llvm-native</a>: for execution on multicore
--   CPUs</li>
--   <li><a>accelerate-llvm-ptx</a>: for execution on NVIDIA CUDA-capable
--   GPUs</li>
--   </ul>
--   
--   <ul>
--   <li><i><i>Tips:</i></i></li>
--   </ul>
--   
--   <ul>
--   <li>Since <a>Acc</a> represents embedded computations that will only
--   be executed when evaluated by a backend, we can programatically
--   generate these computations using the meta language Haskell; for
--   example, unrolling loops or embedding input values into the generated
--   code.</li>
--   <li>It is usually best to keep all intermediate computations in
--   <a>Acc</a>, and only <tt>run</tt> the computation at the very end to
--   produce the final result. This enables optimisations between
--   intermediate results (e.g. array fusion) and, if the target
--   architecture has a separate memory space as is the case of GPUs, to
--   prevent excessive data transfers.</li>
--   </ul>
data Acc a :: * -> *

-- | <a>Arrays</a> consists of nested tuples of individual <a>Array</a>s,
--   currently up to 15-elements wide. Accelerate computations can thereby
--   return multiple results.
class (Typeable * a, Typeable * (ArrRepr a)) => Arrays a

-- | Compile and run a complete embedded array program.
--   
--   NOTE:
--   
--   <ol>
--   <li>It is recommended that you use <a>run1</a> whenever possible.</li>
--   <li>It is *not* safe to call <a>run</a> concurrently from different
--   threads. Instead, use <a>createTarget</a> and <a>runWith</a> so that
--   each thread executes using its own thread gang.</li>
--   </ol>
run :: Arrays a => Acc a -> a

-- | As <a>run</a>, but execute using the specified target (thread gang).
runWith :: Arrays a => Native -> Acc a -> a

-- | Prepare and execute an embedded array program of one argument.
--   
--   This function can be used to improve performance in cases where the
--   array program is constant between invocations, because it enables us
--   to bypass front-end conversion stages and move directly to the
--   execution phase. If you have a computation applied repeatedly to
--   different input data, use this, specifying any changing aspects of the
--   computation via the input parameter. If the function is only evaluated
--   once, this is equivalent to <a>run</a>.
--   
--   To use <a>run1</a> effectively you must express your program as a
--   function of one argument. If your program takes more than one
--   argument, you can use <a>lift</a> and <a>unlift</a> to tuple up the
--   arguments.
--   
--   At an example, once your program is expressed as a function of one
--   argument, instead of the usual:
--   
--   <pre>
--   step :: Acc (Vector a) -&gt; Acc (Vector b)
--   step = ...
--   
--   simulate :: Vector a -&gt; Vector b
--   simulate xs = run $ step (use xs)
--   </pre>
--   
--   Instead write:
--   
--   <pre>
--   simulate xs = run1 step xs
--   </pre>
--   
--   You can use the debugging options to check whether this is working
--   successfully by, for example, observing no output from the
--   <tt>-ddump-cc</tt> flag at the second and subsequent invocations.
--   
--   See the programs in the 'accelerate-examples' package for examples.
run1 :: (Arrays a, Arrays b) => (Acc a -> Acc b) -> a -> b

-- | As <a>run1</a>, but execute using the specified target (thread gang).
run1With :: (Arrays a, Arrays b) => Native -> (Acc a -> Acc b) -> a -> b

-- | Stream a lazily read list of input arrays through the given program,
--   collecting results as we go.
stream :: (Arrays a, Arrays b) => (Acc a -> Acc b) -> [a] -> [b]

-- | As <a>stream</a>, but execute using the specified target (thread
--   gang).
streamWith :: (Arrays a, Arrays b) => Native -> (Acc a -> Acc b) -> [a] -> [b]
data Async a :: * -> *

-- | Block the calling thread until the computation completes, then return
--   the result.
wait :: Async a -> IO a

-- | Test whether the asynchronous computation has already completed. If
--   so, return the result, else <a>Nothing</a>.
poll :: Async a -> IO (Maybe a)

-- | Cancel a running asynchronous computation.
cancel :: Async a -> IO ()

-- | As <a>run</a>, but allow the computation to run asynchronously and
--   return immediately without waiting for the result. The status of the
--   computation can be queried using <a>wait</a>, <a>poll</a>, and
--   <a>cancel</a>.
runAsync :: Arrays a => Acc a -> IO (Async a)

-- | As <a>runAsync</a>, but execute using the specified target (thread
--   gang).
runAsyncWith :: Arrays a => Native -> Acc a -> IO (Async a)

-- | As <a>run1</a>, but execute asynchronously.
run1Async :: (Arrays a, Arrays b) => (Acc a -> Acc b) -> a -> IO (Async b)

-- | As <a>run1Async</a>, but execute using the specified target (thread
--   gang).
run1AsyncWith :: (Arrays a, Arrays b) => Native -> (Acc a -> Acc b) -> a -> IO (Async b)

-- | Native machine code JIT execution target
data Native

-- | The strategy for balancing work amongst the available worker threads.
type Strategy = Gang -> Executable

-- | Create a Native execution target by spawning a worker thread on each
--   of the given capabilities, using the given strategy to load balance
--   the workers.
--   
--   Note that it is *not* safe to use the same target concurrently from
--   different threads; instead, create separate targets for each thread.
createTarget :: [Int] -> Strategy -> IO Native

-- | Execute a computation where threads use work stealing (based on lazy
--   splitting of work stealing queues and exponential backoff) in order to
--   automatically balance the workload amongst themselves. A suitable PPT
--   should be chosen when invoking the continuation in order to balance
--   scheduler overhead with fine-grained function calls.
balancedParIO :: Strategy

-- | Execute a computation without load balancing. Each thread computes an
--   equally sized chunk of the input. No work stealing occurs.
unbalancedParIO :: Strategy
