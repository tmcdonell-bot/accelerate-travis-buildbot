-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | Accelerate backend for NVIDIA GPUs
--   
--   This library implements a backend for the <i>Accelerate</i> language
--   instrumented for parallel execution on CUDA-capable NVIDIA GPUs. For
--   further information, refer to the main <i>Accelerate</i> package:
--   <a>http://hackage.haskell.org/package/accelerate</a>
--   
--   To use this backend you will need:
--   
--   <ol>
--   <li>A CUDA-enabled NVIDIA GPU with, for full functionality, compute
--   capability 1.3 or greater. See the table on Wikipedia for supported
--   GPUs: <a>http://en.wikipedia.org/wiki/CUDA#Supported_GPUs</a></li>
--   <li>The CUDA SDK, available from the NVIDIA Developer Zone:
--   <a>http://developer.nvidia.com/cuda-downloads</a></li>
--   </ol>
--   
--   See the Haddock documentation for additional information related to
--   using this backend.
--   
--   Compile modules that use the CUDA backend with the <tt>-threaded</tt>
--   flag.
@package accelerate-cuda
@version 2.0.0.0


-- | This module implements the CUDA backend for the embedded array
--   language <i>Accelerate</i>. Expressions are on-line translated into
--   CUDA code, compiled, and executed in parallel on the GPU.
--   
--   The accelerate-cuda library is hosted at:
--   <a>https://github.com/AccelerateHS/accelerate-cuda</a>. Comments, bug
--   reports, and patches, are always welcome.
--   
--   <ul>
--   <li><i><i>Data transfer:</i></i></li>
--   </ul>
--   
--   GPUs typically have their own attached memory, which is separate from
--   the computer's main memory. Hence, every <a>use</a> operation implies
--   copying data to the device, and every <a>run</a> operation must copy
--   the results of a computation back to the host.
--   
--   Thus, it is best to keep all computations in the <a>Acc</a>
--   meta-language form and only <a>run</a> the computation once at the
--   end, to avoid transferring (unused) intermediate results.
--   
--   Note that once an array has been transferred to the GPU, it will
--   remain there for as long as that array remains alive on the host. Any
--   subsequent calls to <a>use</a> will find the array cached on the
--   device and not re-transfer the data.
--   
--   <ul>
--   <li><i><i>Caching and performance:</i></i></li>
--   </ul>
--   
--   When the program runs, the <i>Accelerate</i> library evaluates the
--   expression passed to <a>run</a> to make a series of CUDA kernels. Each
--   kernel takes some arrays as inputs and produces arrays as output. Each
--   kernel is a piece of CUDA code that has to be compiled and loaded onto
--   the GPU; this can take a while, so we remember which kernels we have
--   seen before and try to re-use them.
--   
--   The goal is to make kernels that can be re-used. If we don't, the
--   overhead of compiling new kernels can ruin performance.
--   
--   For example, consider the following implementation of the function
--   <a>drop</a> for vectors:
--   
--   <pre>
--   drop :: Elt e =&gt; Exp Int -&gt; Acc (Vector e) -&gt; Acc (Vector e)
--   drop n arr =
--     let n' = the (unit n)
--     in  backpermute (ilift1 (subtract n') (shape arr)) (ilift1 (+ n')) arr
--   </pre>
--   
--   Why did we go to the trouble of converting the <tt>n</tt> value into a
--   scalar array using <a>unit</a>, and then immediately extracting that
--   value using <a>the</a>?
--   
--   We can look at the expression <i>Accelerate</i> sees by evaluating the
--   argument to <a>run</a>. Here is what a typical call to <a>drop</a>
--   evaluates to:
--   
--   <pre>
--   &gt;&gt;&gt; drop (constant 4) (use (fromList (Z:.10) [1..]))
--   let a0 = use (Array (Z :. 10) [1,2,3,4,5,6,7,8,9,10]) in
--   let a1 = unit 4
--   in backpermute
--        (let x0 = Z in x0 :. (indexHead (shape a0)) - (a1!x0))
--        (\x0 -&gt; let x1 = Z in x1 :. (indexHead x0) + (a1!x1))
--        a0
--   </pre>
--   
--   The important thing to note is the line <tt>let a1 = unit 4</tt>. This
--   corresponds to the scalar array we created for the <tt>n</tt> argument
--   to <a>drop</a> and it is <i>outside</i> the call to
--   <a>backpermute</a>. The <a>backpermute</a> function is what turns into
--   a CUDA kernel, and to ensure that we get the same kernel each time we
--   need the arguments to it to remain constant.
--   
--   Let us see what happens if we change <a>drop</a> to instead use its
--   argument <tt>n</tt> directly:
--   
--   <pre>
--   &gt;&gt;&gt; drop (constant 4) (use (fromList (Z:.10) [1..]))
--   let a0 = use (Array (Z :. 10) [1,2,3,4,5,6,7,8,9,10])
--   in backpermute (Z :. -4 + (indexHead (shape a0))) (\x0 -&gt; Z :. 4 + (indexHead x0)) a0
--   </pre>
--   
--   Instead of <tt>n</tt> being outside the call to <a>backpermute</a>, it
--   is now embedded in it. This will defeat <i>Accelerate</i>'s caching of
--   CUDA kernels. Whenever the value of <tt>n</tt> changes, a new kernel
--   will need to be compiled.
--   
--   The rule of thumb is to make sure that any arguments that change are
--   always passed in as arrays, not embedded in the code as constants.
--   
--   How can you tell if you got it wrong? One way is to look at the code
--   directly, as in this example. Another is to use the debugging options
--   provided by the library. See debugging options below.
--   
--   <ul>
--   <li><i><i>Hardware support:</i></i></li>
--   </ul>
--   
--   CUDA devices are categorised into different 'compute capabilities',
--   indicating what operations are supported by the hardware. For example,
--   double precision arithmetic is only supported on devices of compute
--   capability 1.3 or higher.
--   
--   Devices generally perform best when dealing with (tuples of) 32-bit
--   types, so be cautious when introducing 8-, 16-, or 64-bit elements.
--   Keep in mind the size of <a>Int</a> and <a>Word</a> changes depending
--   on the architecture GHC runs on.
--   
--   In particular:
--   
--   <ul>
--   <li><a>Double</a> precision requires compute-1.3.</li>
--   <li><a>Bool</a> is represented internally using <a>Word8</a>,
--   <a>Char</a> by <a>Word32</a>.</li>
--   <li>If the permutation function to <a>permute</a> resolves to
--   non-unique indices, the combination function requires
--   compute-1.1.</li>
--   </ul>
--   
--   <ul>
--   <li><i><i>Debugging options:</i></i></li>
--   </ul>
--   
--   When the library is installed with the <tt>-fdebug</tt> flag, a few
--   extra debugging options are available, input via the command line
--   arguments. The most useful ones are:
--   
--   <ul>
--   <li><tt>-dverbose:</tt> Print some information on the type and
--   capabilities of the GPU being used.</li>
--   <li><tt>-ddump-cc:</tt> Print information about the CUDA kernels as
--   they are compiled and run. Using this option will indicate whether
--   your program is generating the number of kernels that you were
--   expecting. Note that compiled kernels are cached in your home
--   directory, and the generated code will only be displayed if it was not
--   located in this persistent cache. To clear the cache and always print
--   the generated code, use <tt>-fflush-cache</tt> as well.</li>
--   <li><tt>-ddump-exec:</tt> Print each kernel as it is being executed,
--   with timing information.</li>
--   </ul>
--   
--   See the <tt>accelerate-cuda.cabal</tt> file for the full list of
--   options.
--   
--   <ul>
--   <li><i><i>Automatic Graphics Switching on Mac OS X:</i></i></li>
--   </ul>
--   
--   Some Apple computers contain two graphics processors: a low-power
--   integrated graphics chipset, as well as a higher-performance NVIDIA
--   GPU. The latter is of course the one we want to use. Usually Mac OS X
--   detects whenever a program attempts to run a CUDA function and
--   switches to the NVIDIA GPU automatically.
--   
--   However, sometimes this does not work correctly and the problem can
--   manifest in several ways:
--   
--   <ul>
--   <li>The program may report an error such as "No CUDA-capable device is
--   available" or "invalid context handle".</li>
--   <li>For programs that also use OpenGL, the graphics switching might
--   occur and the Accelerate computation complete as expected, but no
--   OpenGL updates appear on screen.</li>
--   </ul>
--   
--   There are several solutions:
--   
--   <ul>
--   <li>Use a tool such as <i>gfxCardStatus</i> to manually select either
--   the integrated or discrete GPU: <a>http://gfx.io</a></li>
--   <li>Disable automatic graphics switching in the Energy Saver pane of
--   System Preferences. Since this disables use of the low-power
--   integrated GPU, this can decrease battery life.</li>
--   <li>When executing the program, disable the RTS clock by appending
--   <tt>+RTS -V0</tt> to the command line arguments. This disables the RTS
--   clock and all timers that depend on it: the context switch timer and
--   the heap profiling timer. Context switches still happen, but
--   deterministically and at a rate much faster than normal. Automatic
--   graphics switching will work correctly, but this method has the
--   disadvantage of reducing performance of the program.</li>
--   </ul>
module Data.Array.Accelerate.CUDA
class (Typeable * a, Typeable * (ArrRepr a)) => Arrays a

-- | Compile and run a complete embedded array program using the CUDA
--   backend. This will select the fastest device available on which to
--   execute computations, based on compute capability and estimated
--   maximum GFLOPS.
--   
--   Note that it is recommended you use <a>run1</a> whenever possible.
run :: Arrays a => Acc a -> a

-- | Prepare and execute an embedded array program of one argument.
--   
--   This function can be used to improve performance in cases where the
--   array program is constant between invocations, because it allows us to
--   bypass all front-end conversion stages and move directly to the
--   execution phase. If you have a computation applied repeatedly to
--   different input data, use this. If the function is only evaluated
--   once, this is equivalent to <a>run</a>.
--   
--   To use <a>run1</a> you must express your program as a function of one
--   argument. If your program takes more than one argument, you can use
--   <a>lift</a> and <a>unlift</a> to tuple up the arguments.
--   
--   At an example, once your program is expressed as a function of one
--   argument, instead of the usual:
--   
--   <pre>
--   step :: Acc (Vector a) -&gt; Acc (Vector b)
--   step = ...
--   
--   simulate :: Vector a -&gt; Vector b
--   simulate xs = run $ step (use xs)
--   </pre>
--   
--   Instead write:
--   
--   <pre>
--   simulate xs = run1 step xs
--   </pre>
--   
--   You can use the debugging options to check whether this is working
--   successfully by, for example, observing no output from the
--   <tt>-ddump-cc</tt> flag at the second and subsequent invocations.
--   
--   See the programs in the 'accelerate-examples' package for examples.
run1 :: (Arrays a, Arrays b) => (Acc a -> Acc b) -> a -> b

-- | As <a>run</a>, but execute using the specified device context rather
--   than using the default, automatically selected device.
--   
--   Contexts passed to this function may all refer to the same device, or
--   to separate devices of differing compute capabilities.
--   
--   Note that each thread has a stack of current contexts, and calling
--   <a>create</a> pushes the new context on top of the stack and makes it
--   current with the calling thread. You should call <a>pop</a> to make
--   the context floating before passing it to <a>runWith</a>, which will
--   make it current for the duration of evaluating the expression. See the
--   CUDA C Programming Guide (G.1) for more information.
runWith :: Arrays a => Context -> Acc a -> a

-- | As <a>run1</a>, but execute in the specified context.
run1With :: (Arrays a, Arrays b) => Context -> (Acc a -> Acc b) -> a -> b

-- | Stream a lazily read list of input arrays through the given program,
--   collecting results as we go.
stream :: (Arrays a, Arrays b) => (Acc a -> Acc b) -> [a] -> [b]

-- | Generate a lazy list from a sequence computation.
streamOut :: Arrays a => Seq [a] -> [a]

-- | As <a>stream</a>, but execute in the specified context.
streamWith :: (Arrays a, Arrays b) => Context -> (Acc a -> Acc b) -> [a] -> [b]
streamOutWith :: Arrays a => Context -> Seq [a] -> [a]
data Async a :: * -> *

-- | Block the calling thread until the computation completes, then return
--   the result.
wait :: Async a -> IO a

-- | Test whether the asynchronous computation has already completed. If
--   so, return the result, else <a>Nothing</a>.
poll :: Async a -> IO (Maybe a)

-- | Cancel a running asynchronous computation.
cancel :: Async a -> IO ()

-- | As <a>run</a>, but allow the computation to continue running in a
--   thread and return immediately without waiting for the result. The
--   status of the computation can be queried using <a>wait</a>,
--   <a>poll</a>, and <a>cancel</a>.
--   
--   Note that a CUDA Context can be active on only one host thread at a
--   time. If you want to execute multiple computations in parallel, use
--   <a>runAsyncWith</a>.
runAsync :: Arrays a => Acc a -> IO (Async a)

-- | As <a>run1</a>, but the computation is executed asynchronously.
run1Async :: (Arrays a, Arrays b) => (Acc a -> Acc b) -> a -> IO (Async b)

-- | As <a>runWith</a>, but execute asynchronously. Be sure not to destroy
--   the context, or attempt to attach it to a different host thread,
--   before all outstanding operations have completed.
runAsyncWith :: Arrays a => Context -> Acc a -> IO (Async a)

-- | As <a>run1With</a>, but execute asynchronously.
run1AsyncWith :: (Arrays a, Arrays b) => Context -> (Acc a -> Acc b) -> a -> IO (Async b)

-- | The execution context
data Context

-- | Create a new CUDA context associated with the calling thread
create :: Device -> [ContextFlag] -> IO Context

-- | Destroy the specified context. This will fail if the context is more
--   than single attachment.
destroy :: Context -> IO ()
unsafeFree :: Arrays arrs => arrs -> IO ()
unsafeFreeWith :: Arrays arrs => Context -> arrs -> IO ()
performGC :: IO ()
performGCWith :: Context -> IO ()


module Data.Array.Accelerate.CUDA.Foreign

-- | Create an Accelerate handle given a device and a cuda context.
--   
--   <pre>
--   AccHandle accelerateCreate(int device, CUcontext ctx);
--   </pre>
accelerateCreate :: Device -> ForeignContext -> IO AccHandle

-- | Releases all resources used by the accelerate library.
--   
--   <pre>
--   void accelerateDestroy(AccHandle hndl);
--   </pre>
accelerateDestroy :: AccHandle -> IO ()

-- | Function callable from foreign code to <a>free</a> a OutputArray
--   returned after executing an Accelerate computation.
--   
--   Once freed, the device pointers associated with an array are no longer
--   valid.
--   
--   <pre>
--   void freeOutput(OutputArray arr);
--   </pre>
freeOutput :: Ptr OutputArray -> IO ()

-- | Free a compiled accelerate program.
--   
--   <pre>
--   void freeProgram(Program prg);
--   </pre>
freeProgram :: StablePtr a -> IO ()

-- | Given the <a>Name</a> of an Accelerate function (a function of type
--   ''Acc a -&gt; Acc b'') generate a a function callable from foreign
--   code with the second argument specifying it's name.
exportAfun :: Name -> String -> Q [Dec]

-- | Given a handle and an Accelerate function, generate an exportable
--   version.
buildExported :: (Arrays a, Arrays b) => AccHandle -> (Acc a -> Acc b) -> IO (StablePtr Afun)

-- | The input required from foreign code.
type InputArray = (ShapeBuffer, DevicePtrBuffer)

-- | A result array from an accelerate program.
type OutputArray = (ShapeBuffer, DevicePtrBuffer, StablePtr EArray)

-- | A foreign buffer that represents a shape as an array of ints.
type ShapeBuffer = Ptr CInt

-- | A buffer of device pointers
type DevicePtrBuffer = Ptr WordPtr

-- | CUDA foreign Acc functions are just CIO functions.
data CUDAForeignAcc as bs
CUDAForeignAcc :: String -> (Stream -> as -> CIO bs) -> CUDAForeignAcc as bs

-- | Gives the executable form of a foreign function if it can be executed
--   by the CUDA backend.
canExecuteAcc :: (Foreign f, Typeable as, Typeable bs) => f as bs -> Maybe (Stream -> as -> CIO bs)

-- | CUDA foreign Exp functions consist of a list of C header files
--   necessary to call the function and the name of the function to call.
data CUDAForeignExp x y
CUDAForeignExp :: [String] -> String -> CUDAForeignExp x y

-- | Gives the foreign function name as a string if it is a foreign Exp
--   function for the CUDA backend.
canExecuteExp :: (Foreign f, Typeable y, Typeable x) => f x y -> Maybe ([String], String)

-- | Get the raw CUDA device pointers associated with an array and call the
--   given continuation.
withDevicePtrs :: Array sh e -> Maybe Stream -> (DevicePtrs (EltRepr e) -> CIO b) -> CIO b

-- | Read a single element from an array at the given row-major index. This
--   is a synchronous operation.
indexArray :: (Shape dim, Elt e) => Array dim e -> Int -> CIO e

-- | Upload an existing array to the device
useArray :: (Shape dim, Elt e) => Array dim e -> CIO ()
useArrayAsync :: (Shape dim, Elt e) => Array dim e -> Maybe Stream -> CIO ()
peekArray :: (Shape dim, Elt e) => Array dim e -> CIO ()
peekArrayAsync :: (Shape dim, Elt e) => Array dim e -> Maybe Stream -> CIO ()
pokeArray :: (Shape dim, Elt e) => Array dim e -> CIO ()
pokeArrayAsync :: (Shape dim, Elt e) => Array dim e -> Maybe Stream -> CIO ()

-- | Copy data between two device arrays. The operation is asynchronous
--   with respect to the host, but will never overlap kernel execution.
copyArray :: (Shape dim, Elt e) => Array dim e -> Array dim e -> CIO ()
copyArrayAsync :: (Shape dim, Elt e) => Array dim e -> Array dim e -> Maybe Stream -> CIO ()
allocateArray :: (Shape dim, Elt e) => dim -> CIO (Array dim e)
newArray :: (Shape sh, Elt e) => sh -> (sh -> e) -> CIO (Array sh e)
data CIO a

-- | A processing stream. All operations in a stream are synchronous and
--   executed in sequence, but operations in different non-default streams
--   may happen out-of-order or concurrently with one another.
--   
--   Use <a>Event</a>s to synchronise operations between streams.
data Stream :: *

-- | Lift a computation from the <a>IO</a> monad.
liftIO :: MonadIO m => forall a. IO a -> m a

-- | Run an IO action within the given Acclerate context
inContext :: Context -> IO a -> IO a

-- | Run an IO action in the default Acclerate context
inDefaultContext :: IO a -> IO a
