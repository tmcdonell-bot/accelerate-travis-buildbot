-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | Accelerate backend generating LLVM
--   
--   This library implements a backend for the <i>Accelerate</i> language
--   which generates LLVM-IR targeting CUDA capable GPUs. For further
--   information, refer to the main <i>Accelerate</i> package:
--   <a>http://hackage.haskell.org/package/accelerate</a>
@package accelerate-llvm-ptx
@version 2.0.0.0


module Data.Array.Accelerate.LLVM.PTX.Foreign
data ForeignAcc f
ForeignAcc :: String -> (Stream -> a -> LLVM PTX b) -> ForeignAcc (a -> b)
data ForeignExp f
ForeignExp :: String -> IRFun1 PTX () (x -> y) -> ForeignExp (x -> y)

-- | The LLVM monad, for executing array computations. This consists of a
--   stack for the LLVM execution context as well as the per-execution
--   target specific state <tt>target</tt>.
data LLVM target a :: * -> * -> *

-- | The PTX execution target for NVIDIA GPUs.
--   
--   The execution target carries state specific for the current execution
--   context. The data here --- device memory and execution streams --- are
--   implicitly tied to this CUDA execution context.
--   
--   Don't store anything here that is independent of the context, for
--   example state related to [persistent] kernel caching should _not_ go
--   here.
data PTX

-- | Lift a computation from the <a>IO</a> monad.
liftIO :: MonadIO m => forall a. IO a -> m a

-- | Lookup the device memory associated with a given host array and do
--   something with it.
withDevicePtr :: (ArrayElt e, ArrayPtrs e ~ Ptr a, Typeable e, Typeable a, Storable a) => ArrayData e -> (DevicePtr a -> LLVM PTX (Maybe Event, r)) -> LLVM PTX r

-- | Copy an array from the remote device to the host. Although the
--   Accelerate program is hyper-strict and will evaluate the computation
--   as soon as any part of it is demanded, the individual array payloads
--   are copied back to the host _only_ as they are demanded by the Haskell
--   program. This has several consequences:
--   
--   <ol>
--   <li>If the device has multiple memcpy engines, only one will be used.
--   The transfers are however associated with a non-default stream.</li>
--   <li>Using <a>seq</a> to force an Array to head-normal form will
--   initiate the computation, but not transfer the results back to the
--   host. Requesting an array element or using <tt>deepseq</tt> to force
--   to normal form is required to actually transfer the data.</li>
--   </ol>
copyToHostLazy :: Arrays arrs => arrs -> LLVM PTX arrs
type Async a = AsyncR PTX a
type Stream = Lifetime Stream
type Event = Lifetime Event
instance Data.Array.Accelerate.LLVM.Foreign.Foreign Data.Array.Accelerate.LLVM.PTX.Target.PTX
instance Data.Array.Accelerate.Array.Sugar.Foreign Data.Array.Accelerate.LLVM.PTX.Foreign.ForeignAcc
instance Data.Array.Accelerate.Array.Sugar.Foreign Data.Array.Accelerate.LLVM.PTX.Foreign.ForeignExp


-- | This module implements a backend for the <i>Accelerate</i> language
--   targeting NVPTX for execution on NVIDIA GPUs. Expressions are on-line
--   translated into LLVM code, which is just-in-time executed in parallel
--   on the GPU.
module Data.Array.Accelerate.LLVM.PTX

-- | Array-valued collective computations
data Acc a :: * -> *
class (Typeable * a, Typeable * (ArrRepr a)) => Arrays a

-- | Compile and run a complete embedded array program.
--   
--   Note that it is recommended that you use <a>run1</a> whenever
--   possible.
run :: Arrays a => Acc a -> a

-- | As <a>run</a>, but execute using the specified target rather than
--   using the default, automatically selected device.
--   
--   Contexts passed to this function may all target to the same device, or
--   to separate devices of differing compute capabilities.
runWith :: Arrays a => PTX -> Acc a -> a

-- | Prepare and execute an embedded array program of one argument.
--   
--   This function can be used to improve performance in cases where the
--   array program is constant between invocations, because it enables us
--   to bypass front-end conversion stages and move directly to the
--   execution phase. If you have a computation applied repeatedly to
--   different input data, use this, specifying any changing aspects of the
--   computation via the input parameter. If the function is only evaluated
--   once, this is equivalent to <a>run</a>.
--   
--   To use <a>run1</a> effectively you must express your program as a
--   function of one argument. If your program takes more than one
--   argument, you can use <a>lift</a> and <a>unlift</a> to tuple up the
--   arguments.
--   
--   At an example, once your program is expressed as a function of one
--   argument, instead of the usual:
--   
--   <pre>
--   step :: Acc (Vector a) -&gt; Acc (Vector b)
--   step = ...
--   
--   simulate :: Vector a -&gt; Vector b
--   simulate xs = run $ step (use xs)
--   </pre>
--   
--   Instead write:
--   
--   <pre>
--   simulate xs = run1 step xs
--   </pre>
--   
--   You can use the debugging options to check whether this is working
--   successfully by, for example, observing no output from the
--   <tt>-ddump-cc</tt> flag at the second and subsequent invocations.
--   
--   See the programs in the 'accelerate-examples' package for examples.
run1 :: (Arrays a, Arrays b) => (Acc a -> Acc b) -> a -> b

-- | As <a>run1</a>, but execute using the specified target rather than
--   using the default, automatically selected device.
run1With :: (Arrays a, Arrays b) => PTX -> (Acc a -> Acc b) -> a -> b

-- | Stream a lazily read list of input arrays through the given program,
--   collecting results as we go.
stream :: (Arrays a, Arrays b) => (Acc a -> Acc b) -> [a] -> [b]

-- | As <a>stream</a>, but execute using the specified target.
streamWith :: (Arrays a, Arrays b) => PTX -> (Acc a -> Acc b) -> [a] -> [b]
data Async a :: * -> *

-- | Block the calling thread until the computation completes, then return
--   the result.
wait :: Async a -> IO a

-- | Test whether the asynchronous computation has already completed. If
--   so, return the result, else <a>Nothing</a>.
poll :: Async a -> IO (Maybe a)

-- | Cancel a running asynchronous computation.
cancel :: Async a -> IO ()

-- | As <a>run</a>, but run the computation asynchronously and return
--   immediately without waiting for the result. The status of the
--   computation can be queried using <a>wait</a>, <a>poll</a>, and
--   <a>cancel</a>.
--   
--   Note that a CUDA context can be active on only one host thread at a
--   time. If you want to execute multiple computations in parallel, on the
--   same or different devices, use <a>runAsyncWith</a>.
runAsync :: Arrays a => Acc a -> IO (Async a)

-- | As <a>runWith</a>, but execute asynchronously. Be sure not to destroy
--   the context, or attempt to attach it to a different host thread,
--   before all outstanding operations have completed.
runAsyncWith :: Arrays a => PTX -> Acc a -> IO (Async a)

-- | As <a>run1</a>, but the computation is executed asynchronously.
run1Async :: (Arrays a, Arrays b) => (Acc a -> Acc b) -> a -> IO (Async b)

-- | As <a>run1With</a>, but execute asynchronously.
run1AsyncWith :: (Arrays a, Arrays b) => PTX -> (Acc a -> Acc b) -> a -> IO (Async b)

-- | The PTX execution target for NVIDIA GPUs.
--   
--   The execution target carries state specific for the current execution
--   context. The data here --- device memory and execution streams --- are
--   implicitly tied to this CUDA execution context.
--   
--   Don't store anything here that is independent of the context, for
--   example state related to [persistent] kernel caching should _not_ go
--   here.
data PTX

-- | Create a new PTX execution target for the given device
createTargetForDevice :: Device -> DeviceProperties -> [ContextFlag] -> IO PTX

-- | Create a PTX execute target for the given device context
createTargetFromContext :: Context -> IO PTX

-- | Configure the default execution target to allocate all future
--   host-side arrays using (CUDA) pinned memory. Any newly allocated
--   arrays will be page-locked and directly accessible from the device,
--   enabling high-speed (asynchronous) DMA.
--   
--   Note that since the amount of available pageable memory will be
--   reduced, overall system performance can suffer.
registerPinnedAllocator :: IO ()

-- | As with <a>registerPinnedAllocator</a>, but configure the given
--   execution context.
registerPinnedAllocatorWith :: PTX -> IO ()
