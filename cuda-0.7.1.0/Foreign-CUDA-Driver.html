<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Foreign.CUDA.Driver</title><link href="ocean.css" rel="stylesheet" type="text/css" title="Ocean" /><script src="haddock-util.js" type="text/javascript"></script><script type="text/javascript">//<![CDATA[
window.onload = function () {pageLoad();setSynopsis("mini_Foreign-CUDA-Driver.html");};
//]]>
</script></head><body><div id="package-header"><ul class="links" id="page-menu"><li><a href="src/Foreign-CUDA-Driver.html">Source</a></li><li><a href="index.html">Contents</a></li><li><a href="doc-index.html">Index</a></li></ul><p class="caption">cuda-0.7.1.0: FFI binding to the CUDA interface for programming NVIDIA GPUs</p></div><div id="content"><div id="module-header"><table class="info"><tr><th>Copyright</th><td>[2009..2015] Trevor L. McDonell</td></tr><tr><th>License</th><td>BSD</td></tr><tr><th>Safe Haskell</th><td>None</td></tr><tr><th>Language</th><td>Haskell98</td></tr></table><p class="caption">Foreign.CUDA.Driver</p></div><div id="description"><p class="caption">Description</p><div class="doc"><p>This module defines an interface to the CUDA driver API. The Driver API
 is a lower-level interface to CUDA devices than that provided by the
 Runtime API. Using the Driver API, the programmer must deal explicitly
 with operations such as initialisation, context management, and loading
 (kernel) modules. Although more difficult to use initially, the Driver
 API provides more control over how CUDA is used. Furthermore, since it
 does not require compiling and linking the program with <code>nvcc</code>, the
 Driver API provides better inter-language compatibility.</p><p>The following is a short tutorial on using the Driver API. The steps can
 be copied into a file, or run directly in <code>ghci</code>, in which case <code>ghci</code>
 should be launched with the option `-fno-ghci-sandbox`. This is because
 CUDA maintains CPU-local state, so operations should always be run from
 a bound thread.</p><dl><dt><em>Using the Driver API</em></dt><dd></dd></dl><p>Before any operation can be performed, the Driver API must be
 initialised:</p><pre class="screen"><code class="prompt">&gt;&gt;&gt; </code><strong class="userinput"><code>import Foreign.CUDA.Driver
</code></strong><code class="prompt">&gt;&gt;&gt; </code><strong class="userinput"><code>initialise []
</code></strong></pre><p>Next, we must select a GPU that we will execute operations on. Each GPU
 is assigned a unique identifier (beginning at zero). We can get a handle
 to a compute device at a given ordinal using the <code><a href="Foreign-CUDA-Driver-Device.html#v:device">device</a></code> operation.
 Given a device handle, we can query the properties of that device using
 <code><a href="Foreign-CUDA-Driver-Device.html#v:props">props</a></code>. The number of available CUDA-capable devices is given via
 <code><a href="Foreign-CUDA-Driver-Device.html#v:count">count</a></code>. For example:</p><pre class="screen"><code class="prompt">&gt;&gt;&gt; </code><strong class="userinput"><code>count
</code></strong>1
<code class="prompt">&gt;&gt;&gt; </code><strong class="userinput"><code>dev0 &lt;- device 0
</code></strong><code class="prompt">&gt;&gt;&gt; </code><strong class="userinput"><code>props dev0
</code></strong>DeviceProperties {deviceName = &quot;GeForce GT 650M&quot;, computeCapability = 3.0, ...}
</pre><p>This package also includes the executable 'nvidia-device-query', which when
 executed displays the key properties of all available devices. See
 <a href="Foreign-CUDA-Driver-Device.html">Foreign.CUDA.Driver.Device</a> for additional operations to query the
 capabilities or status of a device.</p><p>Once you have chosen a device to use, the next step is to create a CUDA
 context. A context is associated with a particular device, and all
 operations, such as memory allocation and kernel execution, take place
 with respect to that context. For example, to <code><a href="Foreign-CUDA-Driver-Context-Base.html#v:create">create</a></code> a new execution
 context on CUDA device 0:</p><pre class="screen"><code class="prompt">&gt;&gt;&gt; </code><strong class="userinput"><code>ctx &lt;- create dev0 []
</code></strong></pre><p>The second argument is a set of <code><a href="Foreign-CUDA-Driver-Context-Base.html#t:ContextFlag">ContextFlag</a></code>s which control how the
 context behaves in various situations, for example, whether or not the
 CPU should actively spin when waiting for results from the GPU
 (<code><a href="Foreign-CUDA-Driver-Context-Base.html#v:SchedSpin">SchedSpin</a></code>), or to yield control to other threads instead
 (<code><a href="Foreign-CUDA-Driver-Context-Base.html#v:SchedYield">SchedYield</a></code>).</p><p>The newly created context is now the <em>active</em> context, and all
 subsequent operations take place within that context. More than one
 context can be created per device, but resources, such as memory
 allocated in the GPU, are unique to each context. The module
 <a href="Foreign-CUDA-Driver-Context.html">Foreign.CUDA.Driver.Context</a> contains operations for managing multiple
 contexts. Some devices allow data to be shared between contexts without
 copying, see <a href="Foreign-CUDA-Driver-Context-Peer.html">Foreign.CUDA.Driver.Context.Peer</a> for more information.</p><p>Once the context is no longer needed, it should be <code><a href="Foreign-CUDA-Driver-Context-Base.html#v:destroy">destroy</a></code>ed in order
 to free up any resources that were allocated to it.</p><pre class="screen"><code class="prompt">&gt;&gt;&gt; </code><strong class="userinput"><code>destroy ctx
</code></strong></pre><p>Each device also has a unique context which is used by the Runtime API.
 This context can be accessed with the module
 <a href="Foreign-CUDA-Driver-Context-Primary.html">Foreign.CUDA.Driver.Context.Primary</a>.</p><dl><dt><em>Executing kernels onto the GPU</em></dt><dd></dd></dl><p>Once the Driver API is initialised and an execution context is created
 on the GPU, we can begin to interact with it.</p><p>At an example, we'll step through executing the CUDA equivalent of the
 following Haskell function, which element-wise adds the elements of two
 arrays:</p><pre class="screen"><code class="prompt">&gt;&gt;&gt; </code><strong class="userinput"><code>vecAdd xs ys = zipWith (+) xs ys
</code></strong></pre><p>The following CUDA kernel can be used to implement this on the GPU:</p><pre>extern &quot;C&quot; __global__ void vecAdd(float *xs, float *ys, float *zs, int N)
{
    int ix = blockIdx.x * blockDim.x + threadIdx.x;

    if ( ix &lt; N ) {
        zs[ix] = xs[ix] + ys[ix];
    }
}</pre><p>Here, the <code>__global__</code> keyword marks the function as a kernel that
 should be computed on the GPU in data parallel. When we execute this
 function on the GPU, (at least) <em>N</em> threads will execute <em>N</em> individual
 instances of the kernel function <code>vecAdd</code>. Each thread will operate on
 a single element of each input array to create a single value in the
 result. See the CUDA programming guide for more details.</p><p>We can save this to a file `vector_add.cu`, and compile it using <code>nvcc</code>
 into a form that we can then load onto the GPU and execute:</p><pre>$ nvcc --ptx vector_add.cu</pre><p>The module <a href="Foreign-CUDA-Driver-Module.html">Foreign.CUDA.Driver.Module</a> contains functions for loading
 the resulting .ptx file (or .cubin files) into the running program.</p><pre class="screen"><code class="prompt">&gt;&gt;&gt; </code><strong class="userinput"><code>mdl &lt;- loadFile &quot;vector_add.ptx&quot;
</code></strong></pre><p>Once finished with the module, it is also a good idea to <code><a href="Foreign-CUDA-Driver-Module-Base.html#v:unload">unload</a></code> it.</p><p>Modules may export kernel functions, global variables, and texture
 references. Before we can execute our function, we need to look it up in
 the module by name.</p><pre class="screen"><code class="prompt">&gt;&gt;&gt; </code><strong class="userinput"><code>vecAdd &lt;- getFun mdl &quot;vecAdd&quot;
</code></strong></pre><p>Given this reference to our kernel function, we are almost ready to
 execute it on the device using <code><a href="Foreign-CUDA-Driver-Exec.html#v:launchKernel">launchKernel</a></code>, but first, we must create
 some data that we can execute the function on.</p><dl><dt><em>Transferring data to and from the GPU</em></dt><dd></dd></dl><p>GPUs typically have their own memory which is separate from the CPU's
 memory, and we need to explicitly copy data back and forth between these
 two regions. The module <a href="Foreign-CUDA-Driver-Marshal.html">Foreign.CUDA.Driver.Marshal</a> provides functions
 for allocating memory on the GPU, and copying data between the CPU and
 GPU, as well as directly between multiple GPUs.</p><p>For simplicity, we'll use standard Haskell lists for our input and
 output data structure. Note however that this will have significantly
 lower effective bandwidth than reading a single contiguous region of
 memory, so for most practical purposes you will want to use some kind of
 unboxed array.</p><pre class="screen"><code class="prompt">&gt;&gt;&gt; </code><strong class="userinput"><code>let xs = [1..1024]   :: [Float]
</code></strong><code class="prompt">&gt;&gt;&gt; </code><strong class="userinput"><code>let ys = [2,4..2048] :: [Float]
</code></strong></pre><p>In CUDA, like C, all memory management is explicit, and arrays on the
 device must be explicitly allocated and freed. As mentioned previously,
 data transfer is also explicit. However, we do provide convenience
 functions for combined allocation and marshalling, as well as bracketed
 operations.</p><pre class="screen"><code class="prompt">&gt;&gt;&gt; </code><strong class="userinput"><code>xs_dev &lt;- newListArray xs
</code></strong><code class="prompt">&gt;&gt;&gt; </code><strong class="userinput"><code>ys_dev &lt;- newListArray ys
</code></strong><code class="prompt">&gt;&gt;&gt; </code><strong class="userinput"><code>zs_dev &lt;- mallocArray 1024 :: IO (DevicePtr Float)
</code></strong></pre><p>After executing the kernel (see next section), we transfer the result
 back to the host, and free the memory that was allocated on the GPU.</p><pre class="screen"><code class="prompt">&gt;&gt;&gt; </code><strong class="userinput"><code>zs &lt;- peekListArray 1024 zs_dev
</code></strong><code class="prompt">&gt;&gt;&gt; </code><strong class="userinput"><code>free xs_dev
</code></strong><code class="prompt">&gt;&gt;&gt; </code><strong class="userinput"><code>free ys_dev
</code></strong><code class="prompt">&gt;&gt;&gt; </code><strong class="userinput"><code>free zs_dev
</code></strong></pre><dl><dt><em>Piecing it all together</em></dt><dd></dd></dl><p>Finally, we have everything in place to execute our operation on the
 GPU. Launching a kernel on the GPU consists of creating many threads on
 the GPU which all execute the same function, and each thread has
 a unique identifier in the grid/block hierarchy which can be used to
 identify exactly which element this thread should process (the
 <code>blockIdx</code> and <code>threadIdx</code> parameters that we saw earlier,
 respectively).</p><p>To execute our function, we will use a grid of 4 blocks, each containing
 256 threads. Thus, a total of 1024 threads will be launched, which will
 each compute a single element of the output array (recall that our input
 arrays each have 1024 elements). The module
 <a href="Foreign-CUDA-Analysis-Occupancy.html">Foreign.CUDA.Analysis.Occupancy</a> contains functions to help determine
 the ideal thread block size for a given kernel and GPU combination.</p><pre class="screen"><code class="prompt">&gt;&gt;&gt; </code><strong class="userinput"><code>launchKernel vecAdd (4,1,1) (256,1,1) 0 Nothing [VArg xs_dev, VArg ys_dev, VArg zs_dev, IArg 1024]
</code></strong></pre><p>Note that kernel execution is asynchronous, so we should also wait for
 the operation to complete before attempting to read the results back.</p><pre class="screen"><code class="prompt">&gt;&gt;&gt; </code><strong class="userinput"><code>sync
</code></strong></pre><p>And that's it!</p><dl><dt><em>Next steps</em></dt><dd></dd></dl><p>As mentioned at the end of the previous section, kernels on the GPU are
 executed asynchronously with respect to the host, and other operations
 such as data transfers can also be executed asynchronously. This allows
 the CPU to continue doing other work while the GPU is busy.
 <code><a href="Foreign-CUDA-Driver-Event.html#v:Event">Event</a></code>s can be used to check whether an
 operation has completed yet.</p><p>It is also possible to execute multiple kernels or data transfers
 concurrently with each other, by assigning those operations to different
 execution <code><a href="Foreign-CUDA-Driver-Stream.html#v:Stream">Stream</a></code>s. Used in conjunction with
 <code><a href="Foreign-CUDA-Driver-Event.html#v:Event">Event</a></code>s, operations will be scheduled
 efficiently only once all dependencies (in the form of
 <code><a href="Foreign-CUDA-Driver-Event.html#v:Event">Event</a></code>s) have been cleared.</p><p>See <a href="Foreign-CUDA-Driver-Event.html">Foreign.CUDA.Driver.Event</a> and <a href="Foreign-CUDA-Driver-Stream.html">Foreign.CUDA.Driver.Stream</a> for
 more information on this topic.</p></div></div><div id="interface"><h1>Documentation</h1><div class="top"><p class="src">module <a href="Foreign-CUDA-Ptr.html">Foreign.CUDA.Ptr</a></p></div><div class="top"><p class="src">module <a href="Foreign-CUDA-Driver-Context.html">Foreign.CUDA.Driver.Context</a></p></div><div class="top"><p class="src">module <a href="Foreign-CUDA-Driver-Device.html">Foreign.CUDA.Driver.Device</a></p></div><div class="top"><p class="src">module <a href="Foreign-CUDA-Driver-Error.html">Foreign.CUDA.Driver.Error</a></p></div><div class="top"><p class="src">module <a href="Foreign-CUDA-Driver-Exec.html">Foreign.CUDA.Driver.Exec</a></p></div><div class="top"><p class="src">module <a href="Foreign-CUDA-Driver-Marshal.html">Foreign.CUDA.Driver.Marshal</a></p></div><div class="top"><p class="src">module <a href="Foreign-CUDA-Driver-Module.html">Foreign.CUDA.Driver.Module</a></p></div><div class="top"><p class="src">module <a href="Foreign-CUDA-Driver-Utils.html">Foreign.CUDA.Driver.Utils</a></p></div></div></div><div id="footer"><p>Produced by <a href="http://www.haskell.org/haddock/">Haddock</a> version 2.16.1</p></div></body></html>