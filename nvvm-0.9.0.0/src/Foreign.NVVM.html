<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><link rel="stylesheet" type="text/css" href="style.css" /><script type="text/javascript" src="highlight.js"></script></head><body><pre><span class="hs-comment">--------------------------------------------------------------------------------</span><span>
</span><a name="line-2"></a><span class="hs-comment">-- |</span><span>
</span><a name="line-3"></a><span class="hs-comment">-- Module    : Foreign.NVVM</span><span>
</span><a name="line-4"></a><span class="hs-comment">-- Copyright : [2016] Trevor L. McDonell</span><span>
</span><a name="line-5"></a><span class="hs-comment">-- License   : BSD</span><span>
</span><a name="line-6"></a><span class="hs-comment">--</span><span>
</span><a name="line-7"></a><span class="hs-comment">-- This module defines an interface to the /libNVVM/ library provided by NVIDIA as</span><span>
</span><a name="line-8"></a><span class="hs-comment">-- part of the CUDA toolkit. It compiles NVVM IR, a compiler intermediate</span><span>
</span><a name="line-9"></a><span class="hs-comment">-- representation based on LLVM IR, into PTX code suitable for execution on</span><span>
</span><a name="line-10"></a><span class="hs-comment">-- NVIDIA GPUs. NVVM IR is a subset of LLVM IR, with a set of rules,</span><span>
</span><a name="line-11"></a><span class="hs-comment">-- restrictions, conventions, and intrinsic functions.</span><span>
</span><a name="line-12"></a><span class="hs-comment">--</span><span>
</span><a name="line-13"></a><span class="hs-comment">-- NVIDIA's own 'nvcc' compiler uses NVVM IR and /libNVVM/ internally as part of</span><span>
</span><a name="line-14"></a><span class="hs-comment">-- the CUDA C compilation process. In contrast to the (open-source) NVPTX target</span><span>
</span><a name="line-15"></a><span class="hs-comment">-- included with the standard LLVM toolchain (which also compiles NVVM IR into</span><span>
</span><a name="line-16"></a><span class="hs-comment">-- PTX code), /libNVVM/ includes a set of proprietary optimisation passes, which</span><span>
</span><a name="line-17"></a><span class="hs-comment">-- /may/ result in faster GPU code. More information on NVVM IR can be found</span><span>
</span><a name="line-18"></a><span class="hs-comment">-- here:</span><span>
</span><a name="line-19"></a><span class="hs-comment">--</span><span>
</span><a name="line-20"></a><span class="hs-comment">-- &lt;http://docs.nvidia.com/cuda/nvvm-ir-spec/index.html&gt;</span><span>
</span><a name="line-21"></a><span class="hs-comment">--</span><span>
</span><a name="line-22"></a><span class="hs-comment">-- The following is a short tutorial on using this library. The steps can be</span><span>
</span><a name="line-23"></a><span class="hs-comment">-- copied into a file, or run directly in @ghci@, in which case @ghci@ should be</span><span>
</span><a name="line-24"></a><span class="hs-comment">-- launched with the option @-fno-ghci-sandbox@. This is because CUDA maintains</span><span>
</span><a name="line-25"></a><span class="hs-comment">-- CPU-local state, so operations must be run from a bound thread.</span><span>
</span><a name="line-26"></a><span class="hs-comment">--</span><span>
</span><a name="line-27"></a><span class="hs-comment">-- Note that the focus of this library is the generation of executable PTX code</span><span>
</span><a name="line-28"></a><span class="hs-comment">-- from NVVM IR, so we will additionally need to use the 'cuda' package to</span><span>
</span><a name="line-29"></a><span class="hs-comment">-- control and execute the compiled program. In this tutorial we will go over</span><span>
</span><a name="line-30"></a><span class="hs-comment">-- those steps quickly, but see the 'cuda' package for more information and</span><span>
</span><a name="line-31"></a><span class="hs-comment">-- a similar tutorial:</span><span>
</span><a name="line-32"></a><span class="hs-comment">--</span><span>
</span><a name="line-33"></a><span class="hs-comment">-- &lt;https://hackage.haskell.org/package/cuda&gt;</span><span>
</span><a name="line-34"></a><span class="hs-comment">--</span><span>
</span><a name="line-35"></a><span class="hs-comment">--</span><span>
</span><a name="line-36"></a><span class="hs-comment">-- [/Initialise the CUDA environment/]</span><span>
</span><a name="line-37"></a><span class="hs-comment">--</span><span>
</span><a name="line-38"></a><span class="hs-comment">-- Before any operation can be performed, we must initialise the CUDA Driver</span><span>
</span><a name="line-39"></a><span class="hs-comment">-- API.</span><span>
</span><a name="line-40"></a><span class="hs-comment">--</span><span>
</span><a name="line-41"></a><span class="hs-comment">-- &gt;&gt;&gt; import Foreign.CUDA.Driver as CUDA</span><span>
</span><a name="line-42"></a><span class="hs-comment">-- &gt;&gt;&gt; CUDA.initialise []</span><span>
</span><a name="line-43"></a><span class="hs-comment">--</span><span>
</span><a name="line-44"></a><span class="hs-comment">-- Select a GPU and create an execution context for that device. Each available</span><span>
</span><a name="line-45"></a><span class="hs-comment">-- device is given a unique numeric identifier (beginning at zero). For this</span><span>
</span><a name="line-46"></a><span class="hs-comment">-- example we just select the first device (the default).</span><span>
</span><a name="line-47"></a><span class="hs-comment">--</span><span>
</span><a name="line-48"></a><span class="hs-comment">-- &gt;&gt;&gt; dev0 &lt;- CUDA.device 0</span><span>
</span><a name="line-49"></a><span class="hs-comment">-- &gt;&gt;&gt; prp0 &lt;- CUDA.props dev0</span><span>
</span><a name="line-50"></a><span class="hs-comment">-- &gt;&gt;&gt; ctx0 &lt;- CUDA.create dev0 []</span><span>
</span><a name="line-51"></a><span class="hs-comment">--</span><span>
</span><a name="line-52"></a><span class="hs-comment">-- Remember that once the context is no longer needed, it should be</span><span>
</span><a name="line-53"></a><span class="hs-comment">-- 'Foreign.CUDA.Driver.Context.Base.destroy'ed in order to free up any</span><span>
</span><a name="line-54"></a><span class="hs-comment">-- resources that were allocated into it.</span><span>
</span><a name="line-55"></a><span class="hs-comment">--</span><span>
</span><a name="line-56"></a><span class="hs-comment">-- [/Compiling kernels with NVVM/]</span><span>
</span><a name="line-57"></a><span class="hs-comment">--</span><span>
</span><a name="line-58"></a><span class="hs-comment">-- For this example we will step through executing the equivalent of the</span><span>
</span><a name="line-59"></a><span class="hs-comment">-- following Haskell function, which element-wise adds the elements of two</span><span>
</span><a name="line-60"></a><span class="hs-comment">-- arrays:</span><span>
</span><a name="line-61"></a><span class="hs-comment">--</span><span>
</span><a name="line-62"></a><span class="hs-comment">-- &gt;&gt;&gt; vecAdd xs ys = zipWith (+) xs ys</span><span>
</span><a name="line-63"></a><span class="hs-comment">--</span><span>
</span><a name="line-64"></a><span class="hs-comment">-- The following NVVM IR implements this for the GPU. Note that this example is</span><span>
</span><a name="line-65"></a><span class="hs-comment">-- written using NVVM IR version 1.2 syntax (corresponding to CUDA toolkit 7.5),</span><span>
</span><a name="line-66"></a><span class="hs-comment">-- which is based on LLVM IR version 3.4. The human readable representation of</span><span>
</span><a name="line-67"></a><span class="hs-comment">-- LLVM IR (and by extension NVVM IR) is notorious for changing between</span><span>
</span><a name="line-68"></a><span class="hs-comment">-- releases, whereas the bitcode representation is somewhat more stable. You may</span><span>
</span><a name="line-69"></a><span class="hs-comment">-- wish to keep this in mind for your own programs.</span><span>
</span><a name="line-70"></a><span class="hs-comment">--</span><span>
</span><a name="line-71"></a><span class="hs-comment">-- &gt; target datalayout = &quot;e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v16:16:16-v32:32:32-v64:64:64-v128:128:128-n16:32:64&quot;</span><span>
</span><a name="line-72"></a><span class="hs-comment">-- &gt; target triple = &quot;nvptx64-nvidia-cuda&quot;</span><span>
</span><a name="line-73"></a><span class="hs-comment">-- &gt;</span><span>
</span><a name="line-74"></a><span class="hs-comment">-- &gt; define void @vecAdd(float* %A, float* %B, float* %C) {</span><span>
</span><a name="line-75"></a><span class="hs-comment">-- &gt; entry:</span><span>
</span><a name="line-76"></a><span class="hs-comment">-- &gt;   ; What is my ID?</span><span>
</span><a name="line-77"></a><span class="hs-comment">-- &gt;   %id = tail call i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind</span><span>
</span><a name="line-78"></a><span class="hs-comment">-- &gt;</span><span>
</span><a name="line-79"></a><span class="hs-comment">-- &gt;   ; Compute pointers into A, B, and C</span><span>
</span><a name="line-80"></a><span class="hs-comment">-- &gt;   %ptrA = getelementptr float* %A, i32 %id</span><span>
</span><a name="line-81"></a><span class="hs-comment">-- &gt;   %ptrB = getelementptr float* %B, i32 %id</span><span>
</span><a name="line-82"></a><span class="hs-comment">-- &gt;   %ptrC = getelementptr float* %C, i32 %id</span><span>
</span><a name="line-83"></a><span class="hs-comment">-- &gt;</span><span>
</span><a name="line-84"></a><span class="hs-comment">-- &gt;   ; Read A, B</span><span>
</span><a name="line-85"></a><span class="hs-comment">-- &gt;   %valA = load float* %ptrA, align 4</span><span>
</span><a name="line-86"></a><span class="hs-comment">-- &gt;   %valB = load float* %ptrB, align 4</span><span>
</span><a name="line-87"></a><span class="hs-comment">-- &gt;</span><span>
</span><a name="line-88"></a><span class="hs-comment">-- &gt;   ; Compute C = A + B</span><span>
</span><a name="line-89"></a><span class="hs-comment">-- &gt;   %valC = fadd float %valA, %valB</span><span>
</span><a name="line-90"></a><span class="hs-comment">-- &gt;</span><span>
</span><a name="line-91"></a><span class="hs-comment">-- &gt;   ; Store back to C</span><span>
</span><a name="line-92"></a><span class="hs-comment">-- &gt;   store float %valC, float* %ptrC, align 4</span><span>
</span><a name="line-93"></a><span class="hs-comment">-- &gt;</span><span>
</span><a name="line-94"></a><span class="hs-comment">-- &gt;   ret void</span><span>
</span><a name="line-95"></a><span class="hs-comment">-- &gt; }</span><span>
</span><a name="line-96"></a><span class="hs-comment">-- &gt;</span><span>
</span><a name="line-97"></a><span class="hs-comment">-- &gt; ; Intrinsic to read threadIdx.x</span><span>
</span><a name="line-98"></a><span class="hs-comment">-- &gt; declare i32 @llvm.nvvm.read.ptx.sreg.tid.x() readnone nounwind</span><span>
</span><a name="line-99"></a><span class="hs-comment">-- &gt;</span><span>
</span><a name="line-100"></a><span class="hs-comment">-- &gt; !nvvm.annotations = !{!0}</span><span>
</span><a name="line-101"></a><span class="hs-comment">-- &gt; !0 = metadata !{void (float*, float*, float*)* @vecAdd, metadata !&quot;kernel&quot;, i64 1}</span><span>
</span><a name="line-102"></a><span class="hs-comment">--</span><span>
</span><a name="line-103"></a><span class="hs-comment">-- For reference, in CUDA this kernel would have been written as:</span><span>
</span><a name="line-104"></a><span class="hs-comment">--</span><span>
</span><a name="line-105"></a><span class="hs-comment">-- &gt; extern &quot;C&quot; __global__ void vecAdd(float *xs, float* ys, float *zs)</span><span>
</span><a name="line-106"></a><span class="hs-comment">-- &gt; {</span><span>
</span><a name="line-107"></a><span class="hs-comment">-- &gt;     int ix = threadIdx.x;</span><span>
</span><a name="line-108"></a><span class="hs-comment">-- &gt;</span><span>
</span><a name="line-109"></a><span class="hs-comment">-- &gt;     zs[ix] = xs[ix] + ys[ix];</span><span>
</span><a name="line-110"></a><span class="hs-comment">-- &gt; }</span><span>
</span><a name="line-111"></a><span class="hs-comment">--</span><span>
</span><a name="line-112"></a><span class="hs-comment">-- The NVVM IR can be stored directly in the program as a @[Byte]String@, but</span><span>
</span><a name="line-113"></a><span class="hs-comment">-- here I will assume that it is saved to a file @vector_add.ll@:</span><span>
</span><a name="line-114"></a><span class="hs-comment">--</span><span>
</span><a name="line-115"></a><span class="hs-comment">-- &gt;&gt;&gt; import Data.ByteString as B</span><span>
</span><a name="line-116"></a><span class="hs-comment">-- &gt;&gt;&gt; ll &lt;- B.readFile &quot;vector_add.ll&quot;</span><span>
</span><a name="line-117"></a><span class="hs-comment">--</span><span>
</span><a name="line-118"></a><span class="hs-comment">-- Now we can use NVVM to compile this into PTX code:</span><span>
</span><a name="line-119"></a><span class="hs-comment">--</span><span>
</span><a name="line-120"></a><span class="hs-comment">-- &gt;&gt;&gt; import Foreign.NVVM as NVVM</span><span>
</span><a name="line-121"></a><span class="hs-comment">-- &gt;&gt;&gt; ptx &lt;- NVVM.compileModule &quot;vecAdd&quot; ll [ NVVM.Target (CUDA.computeCapability prp0) ]</span><span>
</span><a name="line-122"></a><span class="hs-comment">--</span><span>
</span><a name="line-123"></a><span class="hs-comment">-- Notice that we asked NVVM to specialise the generated PTX code for our</span><span>
</span><a name="line-124"></a><span class="hs-comment">-- current device. By default the code will be compiled for compute capability</span><span>
</span><a name="line-125"></a><span class="hs-comment">-- 2.0 (the earliest supported target).</span><span>
</span><a name="line-126"></a><span class="hs-comment">--</span><span>
</span><a name="line-127"></a><span class="hs-comment">-- The generated PTX code can then be loaded into the current CUDA execution</span><span>
</span><a name="line-128"></a><span class="hs-comment">-- context, from which we can extract a reference to the GPU kernel that we will</span><span>
</span><a name="line-129"></a><span class="hs-comment">-- later execute.</span><span>
</span><a name="line-130"></a><span class="hs-comment">--</span><span>
</span><a name="line-131"></a><span class="hs-comment">-- &gt;&gt;&gt; mdl    &lt;- CUDA.loadData (NVVM.compileResult ptx)</span><span>
</span><a name="line-132"></a><span class="hs-comment">-- &gt;&gt;&gt; vecAdd &lt;- CUDA.getFun mdl &quot;vecAdd&quot;</span><span>
</span><a name="line-133"></a><span class="hs-comment">--</span><span>
</span><a name="line-134"></a><span class="hs-comment">-- After we are finished with the module, it is a good idea to</span><span>
</span><a name="line-135"></a><span class="hs-comment">-- 'Foreign.CUDA.Driver.Module.Base.unload' it in order to free any resources it</span><span>
</span><a name="line-136"></a><span class="hs-comment">-- used.</span><span>
</span><a name="line-137"></a><span class="hs-comment">--</span><span>
</span><a name="line-138"></a><span class="hs-comment">-- [/Executing the kernel/]</span><span>
</span><a name="line-139"></a><span class="hs-comment">--</span><span>
</span><a name="line-140"></a><span class="hs-comment">-- Executing the 'vecAdd' kernel now proceeds exactly like executing any other</span><span>
</span><a name="line-141"></a><span class="hs-comment">-- kernel function using the CUDA Driver API. The following is a quick overview;</span><span>
</span><a name="line-142"></a><span class="hs-comment">-- see the tutorial in the 'cuda' package for more information.</span><span>
</span><a name="line-143"></a><span class="hs-comment">--</span><span>
</span><a name="line-144"></a><span class="hs-comment">-- First, generate some data and copy it to the device. We also allocate an</span><span>
</span><a name="line-145"></a><span class="hs-comment">-- (uninitialised) array on the device to store the results.</span><span>
</span><a name="line-146"></a><span class="hs-comment">--</span><span>
</span><a name="line-147"></a><span class="hs-comment">-- &gt;&gt;&gt; let xs = [1..256]   :: [Float]</span><span>
</span><a name="line-148"></a><span class="hs-comment">-- &gt;&gt;&gt; let ys = [2,4..512] :: [Float]</span><span>
</span><a name="line-149"></a><span class="hs-comment">-- &gt;&gt;&gt; xs_dev &lt;- CUDA.newListArray xs</span><span>
</span><a name="line-150"></a><span class="hs-comment">-- &gt;&gt;&gt; ys_dev &lt;- CUDA.newListArray ys</span><span>
</span><a name="line-151"></a><span class="hs-comment">-- &gt;&gt;&gt; zs_dev &lt;- CUDA.mallocArray 256 :: IO (CUDA.DevicePtr Float)</span><span>
</span><a name="line-152"></a><span class="hs-comment">--</span><span>
</span><a name="line-153"></a><span class="hs-comment">-- For this simple kernel we execute it using a single (one dimensional) thread</span><span>
</span><a name="line-154"></a><span class="hs-comment">-- block, with one thread computing each element of the output.</span><span>
</span><a name="line-155"></a><span class="hs-comment">--</span><span>
</span><a name="line-156"></a><span class="hs-comment">-- &gt;&gt;&gt; CUDA.launchKernel vecAdd (1,1,1) (256,1,1) 0 Nothing [CUDA.VArg xs_dev, CUDA.VArg ys_dev, CUDA.VArg zs_dev]</span><span>
</span><a name="line-157"></a><span class="hs-comment">--</span><span>
</span><a name="line-158"></a><span class="hs-comment">-- Finally, we can copy the results back to the host, and deallocate the arrays</span><span>
</span><a name="line-159"></a><span class="hs-comment">-- from the GPU.</span><span>
</span><a name="line-160"></a><span class="hs-comment">--</span><span>
</span><a name="line-161"></a><span class="hs-comment">-- &gt;&gt;&gt; zs &lt;- CUDA.peekListArray 256 zs_dev</span><span>
</span><a name="line-162"></a><span class="hs-comment">-- &gt;&gt;&gt; CUDA.free xs_dev</span><span>
</span><a name="line-163"></a><span class="hs-comment">-- &gt;&gt;&gt; CUDA.free ys_dev</span><span>
</span><a name="line-164"></a><span class="hs-comment">-- &gt;&gt;&gt; CUDA.free zs_dev</span><span>
</span><a name="line-165"></a><span class="hs-comment">--</span><span>
</span><a name="line-166"></a><span class="hs-comment">--</span><span>
</span><a name="line-167"></a><span class="hs-comment">-- [/Next steps/]</span><span>
</span><a name="line-168"></a><span class="hs-comment">--</span><span>
</span><a name="line-169"></a><span class="hs-comment">-- The library also provides functions for compiling several NVVM IR sources</span><span>
</span><a name="line-170"></a><span class="hs-comment">-- into a single module. In particular this is useful when linking against</span><span>
</span><a name="line-171"></a><span class="hs-comment">-- @libdevice@, a standard library of functions in NVVM IR which implement, for</span><span>
</span><a name="line-172"></a><span class="hs-comment">-- example, math primitives and bitwise operations. More information on</span><span>
</span><a name="line-173"></a><span class="hs-comment">-- @libdevice@ can be found here:</span><span>
</span><a name="line-174"></a><span class="hs-comment">--</span><span>
</span><a name="line-175"></a><span class="hs-comment">-- &lt;http://docs.nvidia.com/cuda/libdevice-users-guide/index.html&gt;</span><span>
</span><a name="line-176"></a><span class="hs-comment">--</span><span>
</span><a name="line-177"></a><span class="hs-comment">--------------------------------------------------------------------------------</span><span>
</span><a name="line-178"></a><span>
</span><a name="line-179"></a><span class="hs-keyword">module</span><span> </span><span class="hs-identifier">Foreign.NVVM</span><span> </span><span class="hs-special">(</span><span>
</span><a name="line-180"></a><span>
</span><a name="line-181"></a><span>  </span><span class="hs-keyword">module</span><span> </span><a href="Foreign.NVVM.Compile.html"><span class="hs-identifier">Foreign.NVVM.Compile</span></a><span class="hs-special">,</span><span>
</span><a name="line-182"></a><span>  </span><span class="hs-keyword">module</span><span> </span><a href="Foreign.NVVM.Error.html"><span class="hs-identifier">Foreign.NVVM.Error</span></a><span class="hs-special">,</span><span>
</span><a name="line-183"></a><span>  </span><span class="hs-keyword">module</span><span> </span><a href="Foreign.NVVM.Info.html"><span class="hs-identifier">Foreign.NVVM.Info</span></a><span class="hs-special">,</span><span>
</span><a name="line-184"></a><span>
</span><a name="line-185"></a><span class="hs-special">)</span><span> </span><span class="hs-keyword">where</span><span>
</span><a name="line-186"></a><span>
</span><a name="line-187"></a><span class="hs-keyword">import</span><span> </span><a href="Foreign.NVVM.Compile.html"><span class="hs-identifier">Foreign.NVVM.Compile</span></a><span>
</span><a name="line-188"></a><span class="hs-keyword">import</span><span> </span><a href="Foreign.NVVM.Error.html"><span class="hs-identifier">Foreign.NVVM.Error</span></a><span>
</span><a name="line-189"></a><span class="hs-keyword">import</span><span> </span><a href="Foreign.NVVM.Info.html"><span class="hs-identifier">Foreign.NVVM.Info</span></a><span>
</span><a name="line-190"></a><span>
</span><a name="line-191"></a></pre></body></html>