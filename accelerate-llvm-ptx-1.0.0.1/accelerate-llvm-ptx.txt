-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | Accelerate backend generating LLVM
--   
--   This library implements a backend for the <i>Accelerate</i> language
--   which generates LLVM-IR targeting CUDA capable GPUs. For further
--   information, refer to the main <a>accelerate</a> package.
--   
--   <ul>
--   <li><i><i>Dependencies</i></i></li>
--   </ul>
--   
--   Haskell dependencies are available from Hackage. The following
--   external libraries are alse required:
--   
--   <ul>
--   <li><a>LLVM</a></li>
--   <li><a>CUDA</a></li>
--   </ul>
--   
--   <ul>
--   <li><i><i>Installing LLVM</i></i></li>
--   </ul>
--   
--   <i>Homebrew</i>
--   
--   Example using Homebrew on macOS:
--   
--   <pre>
--   brew install llvm-hs/homebrew-llvm/llvm-4.0
--   </pre>
--   
--   <i>Debian &amp; Ubuntu</i>
--   
--   For Debian/Ubuntu based Linux distributions, the LLVM.org website
--   provides binary distribution packages. Check <a>apt.llvm.org</a> for
--   instructions for adding the correct package database for your OS
--   version, and then:
--   
--   <pre>
--   apt-get install llvm-4.0-dev
--   </pre>
--   
--   <i>Building from source</i>
--   
--   If your OS does not have an appropriate LLVM distribution available,
--   you can also build from source. Detailed build instructions are
--   available on <a>LLVM.org</a>. Make sure to include the cmake build
--   options <tt>-DLLVM_BUILD_LLVM_DYLIB=ON -DLLVM_LINK_LLVM_DYLIB=ON</tt>
--   so that the <tt>libLLVM</tt> shared library will be built. Also ensure
--   that the <tt>LLVM_TARGETS_TO_BUILD</tt> option includes the
--   <tt>NVPTX</tt> target (if not specified all targets are built).
--   
--   <ul>
--   <li><i><i>Installing accelerate-llvm</i></i></li>
--   </ul>
--   
--   To use <tt>accelerate-llvm</tt> it is important that the
--   <tt>llvm-hs</tt> package is installed against the <tt>libLLVM</tt>
--   shared library, rather than statically linked, so that we can use LLVM
--   from GHCi and Template Haskell. This is the default configuration, but
--   you can also enforce this explicitly by adding the following to your
--   <tt>stack.yaml</tt> file:
--   
--   <pre>
--   flags:
--     llvm-hs:
--       shared-llvm: true
--   </pre>
--   
--   Or by specifying the <tt>shared-llvm</tt> flag to cabal:
--   
--   <pre>
--   cabal install llvm-hs -fshared-llvm
--   </pre>
@package accelerate-llvm-ptx
@version 1.0.0.1


module Data.Array.Accelerate.LLVM.PTX.Foreign
data ForeignAcc f
[ForeignAcc] :: String -> (Stream -> a -> LLVM PTX b) -> ForeignAcc (a -> b)
data ForeignExp f
[ForeignExp] :: String -> IRFun1 PTX () (x -> y) -> ForeignExp (x -> y)

-- | The LLVM monad, for executing array computations. This consists of a
--   stack for the LLVM execution context as well as the per-execution
--   target specific state <tt>target</tt>.
data LLVM target a :: * -> * -> *

-- | The PTX execution target for NVIDIA GPUs.
--   
--   The execution target carries state specific for the current execution
--   context. The data here --- device memory and execution streams --- are
--   implicitly tied to this CUDA execution context.
--   
--   Don't store anything here that is independent of the context, for
--   example state related to [persistent] kernel caching should _not_ go
--   here.
data PTX
PTX :: {-# UNPACK #-} !Context -> {-# UNPACK #-} !MemoryTable -> {-# UNPACK #-} !KernelTable -> {-# UNPACK #-} !Reservoir -> {-# UNPACK #-} !Executable -> PTX
[ptxContext] :: PTX -> {-# UNPACK #-} !Context
[ptxMemoryTable] :: PTX -> {-# UNPACK #-} !MemoryTable
[ptxKernelTable] :: PTX -> {-# UNPACK #-} !KernelTable
[ptxStreamReservoir] :: PTX -> {-# UNPACK #-} !Reservoir
[fillP] :: PTX -> {-# UNPACK #-} !Executable

-- | An execution context, which is tied to a specific device and CUDA
--   execution context.
data Context
Context :: {-# UNPACK #-} !DeviceProperties -> {-# UNPACK #-} !(Lifetime Context) -> Context
[deviceProperties] :: Context -> {-# UNPACK #-} !DeviceProperties
[deviceContext] :: Context -> {-# UNPACK #-} !(Lifetime Context)

-- | Lift a computation from the <a>IO</a> monad.
liftIO :: MonadIO m => forall a. IO a -> m a

-- | Lookup the device memory associated with a given host array and do
--   something with it.
withDevicePtr :: (ArrayElt e, ArrayPtrs e ~ Ptr a, Typeable e, Typeable a, Storable a) => ArrayData e -> (DevicePtr a -> LLVM PTX (Maybe Event, r)) -> LLVM PTX r

-- | Copy an array from the remote device to the host. Although the
--   Accelerate program is hyper-strict and will evaluate the computation
--   as soon as any part of it is demanded, the individual array payloads
--   are copied back to the host _only_ as they are demanded by the Haskell
--   program. This has several consequences:
--   
--   <ol>
--   <li>If the device has multiple memcpy engines, only one will be used.
--   The transfers are however associated with a non-default stream.</li>
--   <li>Using <a>seq</a> to force an Array to head-normal form will
--   initiate the computation, but not transfer the results back to the
--   host. Requesting an array element or using <tt>deepseq</tt> to force
--   to normal form is required to actually transfer the data.</li>
--   </ol>
copyToHostLazy :: Arrays arrs => arrs -> LLVM PTX arrs

-- | Clone an array into a newly allocated array on the device.
cloneArrayAsync :: (Shape sh, Elt e) => Stream -> Array sh e -> LLVM PTX (Array sh e)
type Async a = AsyncR PTX a

-- | A <a>Stream</a> represents an independent sequence of computations
--   executed on the GPU. Operations in different streams may be executed
--   concurrently with each other, but operations in the same stream can
--   never overlap. <a>Event</a>s can be used for efficient cross-stream
--   synchronisation.
type Stream = Lifetime Stream

-- | Events can be used for efficient device-side synchronisation between
--   execution streams and between the host.
type Event = Lifetime Event
instance Data.Array.Accelerate.LLVM.Foreign.Foreign Data.Array.Accelerate.LLVM.PTX.Target.PTX
instance Data.Array.Accelerate.Array.Sugar.Foreign Data.Array.Accelerate.LLVM.PTX.Foreign.ForeignAcc
instance Data.Array.Accelerate.Array.Sugar.Foreign Data.Array.Accelerate.LLVM.PTX.Foreign.ForeignExp


-- | This module implements a backend for the <i>Accelerate</i> language
--   targeting NVPTX for execution on NVIDIA GPUs. Expressions are on-line
--   translated into LLVM code, which is just-in-time executed in parallel
--   on the GPU.
module Data.Array.Accelerate.LLVM.PTX

-- | Accelerate is an <i>embedded language</i> that distinguishes between
--   vanilla arrays (e.g. in Haskell memory on the CPU) and embedded arrays
--   (e.g. in device memory on a GPU), as well as the computations on both
--   of these. Since Accelerate is an embedded language, programs written
--   in Accelerate are not compiled by the Haskell compiler (GHC). Rather,
--   each Accelerate backend is a <i>runtime compiler</i> which generates
--   and executes parallel SIMD code of the target language at application
--   <i>runtime</i>.
--   
--   The type constructor <a>Acc</a> represents embedded collective array
--   operations. A term of type <tt>Acc a</tt> is an Accelerate program
--   which, once executed, will produce a value of type <tt>a</tt> (an
--   <a>Array</a> or a tuple of <a>Arrays</a>). Collective operations of
--   type <tt>Acc a</tt> comprise many <i>scalar expressions</i>, wrapped
--   in type constructor <a>Exp</a>, which will be executed in parallel.
--   Although collective operations comprise many scalar operations
--   executed in parallel, scalar operations <i>cannot</i> initiate new
--   collective operations: this stratification between scalar operations
--   in <a>Exp</a> and array operations in <a>Acc</a> helps statically
--   exclude <i>nested data parallelism</i>, which is difficult to execute
--   efficiently on constrained hardware such as GPUs.
--   
--   For example, to compute a vector dot product we could write:
--   
--   <pre>
--   dotp :: Num a =&gt; Vector a -&gt; Vector a -&gt; Acc (Scalar a)
--   dotp xs ys =
--     let
--         xs' = use xs
--         ys' = use ys
--     in
--     fold (+) 0 ( zipWith (*) xs' ys' )
--   </pre>
--   
--   The function <tt>dotp</tt> consumes two one-dimensional arrays
--   (<a>Vector</a>s) of values, and produces a single (<a>Scalar</a>)
--   result as output. As the return type is wrapped in the type
--   <a>Acc</a>, we see that it is an embedded Accelerate computation - it
--   will be evaluated in the <i>object</i> language of dynamically
--   generated parallel code, rather than the <i>meta</i> language of
--   vanilla Haskell.
--   
--   As the arguments to <tt>dotp</tt> are plain Haskell arrays, to make
--   these available to Accelerate computations they must be embedded with
--   the <a>use</a> function.
--   
--   An Accelerate backend is used to evaluate the embedded computation and
--   return the result back to vanilla Haskell. Calling the <tt>run</tt>
--   function of a backend will generate code for the target architecture,
--   compile, and execute it. For example, the following backends are
--   available:
--   
--   <ul>
--   <li><a>accelerate-llvm-native</a>: for execution on multicore
--   CPUs</li>
--   <li><a>accelerate-llvm-ptx</a>: for execution on NVIDIA CUDA-capable
--   GPUs</li>
--   </ul>
--   
--   See also <a>Exp</a>, which encapsulates embedded <i>scalar</i>
--   computations.
--   
--   <ul>
--   <li><i><i>Fusion:</i></i></li>
--   </ul>
--   
--   Array computations of type <a>Acc</a> will be subject to <i>array
--   fusion</i>; Accelerate will combine individual <a>Acc</a> computations
--   into a single computation, which reduces the number of traversals over
--   the input data and thus improves performance. As such, it is often
--   useful to have some intuition on when fusion should occur.
--   
--   The main idea is to first partition array operations into two
--   categories:
--   
--   <ol>
--   <li>Element-wise operations, such as <a>map</a>, <a>generate</a>, and
--   <a>backpermute</a>. Each element of these operations can be computed
--   independently of all others.</li>
--   <li>Collective operations such as <a>fold</a>, <a>scanl</a>, and
--   <a>stencil</a>. To compute each output element of these operations
--   requires reading multiple elements from the input array(s).</li>
--   </ol>
--   
--   Element-wise operations fuse together whenever the consumer operation
--   uses a single element of the input array. Element-wise operations can
--   both fuse their inputs into themselves, as well be fused into later
--   operations. Both these examples should fuse into a single loop:
--   
--   <pre>
--   map -&gt; reverse -&gt; reshape -&gt; map -&gt; map
--   </pre>
--   
--   <pre>
--   map -&gt; backpermute -&gt;
--                         zipWith -&gt; map
--             generate -&gt;
--   </pre>
--   
--   If the consumer operation uses more than one element of the input
--   array (typically, via <a>generate</a> indexing an array multiple
--   times), then the input array will be completely evaluated first; no
--   fusion occurs in this case, because fusing the first operation into
--   the second implies duplicating work.
--   
--   On the other hand, collective operations can fuse their input arrays
--   into themselves, but on output always evaluate to an array; collective
--   operations will not be fused into a later step. For example:
--   
--   <pre>
--        use -&gt;
--               zipWith -&gt; fold |-&gt; map
--   generate -&gt;
--   </pre>
--   
--   Here the element-wise sequence (<a>use</a> + <a>generate</a> +
--   <a>zipWith</a>) will fuse into a single operation, which then fuses
--   into the collective <a>fold</a> operation. At this point in the
--   program the <a>fold</a> must now be evaluated. In the final step the
--   <a>map</a> reads in the array produced by <a>fold</a>. As there is no
--   fusion between the <a>fold</a> and <a>map</a> steps, this program
--   consists of two "loops"; one for the <a>use</a> + <a>generate</a> +
--   <a>zipWith</a> + <a>fold</a> step, and one for the final <a>map</a>
--   step.
--   
--   You can see how many operations will be executed in the fused program
--   by <a>Show</a>-ing the <a>Acc</a> program, or by using the debugging
--   option <tt>-ddump-dot</tt> to save the program as a graphviz DOT file.
--   
--   As a special note, the operations <a>unzip</a> and <a>reshape</a>,
--   when applied to a real array, are executed in constant time, so in
--   this situation these operations will not be fused.
--   
--   <ul>
--   <li><i><i>Tips:</i></i></li>
--   </ul>
--   
--   <ul>
--   <li>Since <a>Acc</a> represents embedded computations that will only
--   be executed when evaluated by a backend, we can programatically
--   generate these computations using the meta language Haskell; for
--   example, unrolling loops or embedding input values into the generated
--   code.</li>
--   <li>It is usually best to keep all intermediate computations in
--   <a>Acc</a>, and only <tt>run</tt> the computation at the very end to
--   produce the final result. This enables optimisations between
--   intermediate results (e.g. array fusion) and, if the target
--   architecture has a separate memory space as is the case of GPUs, to
--   prevent excessive data transfers.</li>
--   </ul>
data Acc a :: * -> *

-- | <a>Arrays</a> consists of nested tuples of individual <a>Array</a>s,
--   currently up to 15-elements wide. Accelerate computations can thereby
--   return multiple results.
class (Typeable * a, Typeable * (ArrRepr a)) => Arrays a

-- | Compile and run a complete embedded array program.
--   
--   The result is copied back to the host only once the arrays are
--   demanded (or the result is forced to normal form). For results
--   consisting of multiple components (a tuple of arrays or array of
--   tuples) this applies per primitive array. Evaluating the result of
--   <a>run</a> to WHNF will initiate the computation, but does not copy
--   the results back from the device.
--   
--   Note that it is recommended that you use <a>run1</a> whenever
--   possible.
run :: Arrays a => Acc a -> a

-- | As <a>run</a>, but execute using the specified target rather than
--   using the default, automatically selected device.
--   
--   Contexts passed to this function may all target to the same device, or
--   to separate devices of differing compute capabilities.
runWith :: Arrays a => PTX -> Acc a -> a

-- | This is <a>runN</a>, specialised to an array program of one argument.
run1 :: (Arrays a, Arrays b) => (Acc a -> Acc b) -> a -> b

-- | As <a>run1</a>, but execute using the specified target rather than
--   using the default, automatically selected device.
run1With :: (Arrays a, Arrays b) => PTX -> (Acc a -> Acc b) -> a -> b

-- | Prepare and execute an embedded array program.
--   
--   This function can be used to improve performance in cases where the
--   array program is constant between invocations, because it enables us
--   to bypass front-end conversion stages and move directly to the
--   execution phase. If you have a computation applied repeatedly to
--   different input data, use this, specifying any changing aspects of the
--   computation via the input parameters. If the function is only
--   evaluated once, this is equivalent to <a>run</a>.
--   
--   In order to use <a>runN</a> you must express your Accelerate program
--   as a function of array terms:
--   
--   <pre>
--   f :: (Arrays a, Arrays b, ... Arrays c) =&gt; Acc a -&gt; Acc b -&gt; ... -&gt; Acc c
--   </pre>
--   
--   This function then returns the compiled version of <tt>f</tt>:
--   
--   <pre>
--   runN f :: (Arrays a, Arrays b, ... Arrays c) =&gt; a -&gt; b -&gt; ... -&gt; c
--   </pre>
--   
--   At an example, rather than:
--   
--   <pre>
--   step :: Acc (Vector a) -&gt; Acc (Vector b)
--   step = ...
--   
--   simulate :: Vector a -&gt; Vector b
--   simulate xs = run $ step (use xs)
--   </pre>
--   
--   Instead write:
--   
--   <pre>
--   simulate = runN step
--   </pre>
--   
--   You can use the debugging options to check whether this is working
--   successfully. For example, running with the <tt>-ddump-phases</tt>
--   flag should show that the compilation steps only happen once, not on
--   the second and subsequent invocations of <tt>simulate</tt>. Note that
--   this typically relies on GHC knowing that it can lift out the function
--   returned by <a>runN</a> and reuse it.
--   
--   As with <a>run</a>, the resulting array(s) are only copied back to the
--   host once they are actually demanded (forced to normal form). Thus,
--   splitting a program into multiple <a>runN</a> steps does not imply
--   transferring intermediate computations back and forth between host and
--   device. However note that Accelerate is not able to optimise (fuse)
--   across separate <a>runN</a> invocations.
--   
--   See the programs in the 'accelerate-examples' package for examples.
runN :: Afunction f => f -> AfunctionR f

-- | As <a>runN</a>, but execute using the specified target device.
runNWith :: Afunction f => PTX -> f -> AfunctionR f

-- | Stream a lazily read list of input arrays through the given program,
--   collecting results as we go.
stream :: (Arrays a, Arrays b) => (Acc a -> Acc b) -> [a] -> [b]

-- | As <a>stream</a>, but execute using the specified target.
streamWith :: (Arrays a, Arrays b) => PTX -> (Acc a -> Acc b) -> [a] -> [b]
data Async a :: * -> *

-- | Block the calling thread until the computation completes, then return
--   the result.
wait :: Async a -> IO a

-- | Test whether the asynchronous computation has already completed. If
--   so, return the result, else <a>Nothing</a>.
poll :: Async a -> IO (Maybe a)

-- | Cancel a running asynchronous computation.
cancel :: Async a -> IO ()

-- | As <a>run</a>, but run the computation asynchronously and return
--   immediately without waiting for the result. The status of the
--   computation can be queried using <a>wait</a>, <a>poll</a>, and
--   <a>cancel</a>.
--   
--   Note that a CUDA context can be active on only one host thread at a
--   time. If you want to execute multiple computations in parallel, on the
--   same or different devices, use <a>runAsyncWith</a>.
runAsync :: Arrays a => Acc a -> IO (Async a)

-- | As <a>runWith</a>, but execute asynchronously. Be sure not to destroy
--   the context, or attempt to attach it to a different host thread,
--   before all outstanding operations have completed.
runAsyncWith :: Arrays a => PTX -> Acc a -> IO (Async a)

-- | As <a>run1</a>, but the computation is executed asynchronously.
run1Async :: (Arrays a, Arrays b) => (Acc a -> Acc b) -> a -> IO (Async b)

-- | As <a>run1With</a>, but execute asynchronously.
run1AsyncWith :: (Arrays a, Arrays b) => PTX -> (Acc a -> Acc b) -> a -> IO (Async b)

-- | As <a>runN</a>, but execute asynchronously.
runNAsync :: (Afunction f, RunAsync r, AfunctionR f ~ RunAsyncR r) => f -> r

-- | As <a>runNWith</a>, but execute asynchronously.
runNAsyncWith :: (Afunction f, RunAsync r, AfunctionR f ~ RunAsyncR r) => PTX -> f -> r

-- | Ahead-of-time compilation for an embedded array program.
--   
--   This function will generate, compile, and link into the final
--   executable, code to execute the given Accelerate computation <i>at
--   Haskell compile time</i>. This eliminates any runtime overhead
--   associated with the other <tt>run*</tt> operations. The generated code
--   will be compiled for the current (default) GPU architecture.
--   
--   Since the Accelerate program will be generated at Haskell compile
--   time, construction of the Accelerate program, in particular via
--   meta-programming, will be limited to operations available to that
--   phase. Also note that any arrays which are embedded into the program
--   via <a>use</a> will be stored as part of the final executable.
--   
--   <ul>
--   <li><i><i>Note:</i></i></li>
--   </ul>
--   
--   Due to <a>GHC#13587</a>, this currently must be as an <i>untyped</i>
--   splice.
--   
--   The correct type of this function is similar to that of <a>runN</a>:
--   
--   <pre>
--   runQ :: Afunction f =&gt; f -&gt; Q (TExp (AfunctionR f))
--   </pre>
--   
--   Since 1.1.0.0.
runQ :: Afunction f => f -> ExpQ

-- | Ahead-of-time analogue of <a>runNAsync</a>. See <a>runQ</a> for more
--   information.
--   
--   The correct type of this function is:
--   
--   <pre>
--   runQAsync :: (Afunction f, RunAsync r, AfunctionR f ~ RunAsyncR r) =&gt; f -&gt; Q (TExp r)
--   </pre>
--   
--   Since 1.1.0.0.
runQAsync :: Afunction f => f -> ExpQ

-- | The PTX execution target for NVIDIA GPUs.
--   
--   The execution target carries state specific for the current execution
--   context. The data here --- device memory and execution streams --- are
--   implicitly tied to this CUDA execution context.
--   
--   Don't store anything here that is independent of the context, for
--   example state related to [persistent] kernel caching should _not_ go
--   here.
data PTX

-- | Create a new PTX execution target for the given device
createTargetForDevice :: Device -> DeviceProperties -> [ContextFlag] -> IO PTX

-- | Create a PTX execute target for the given device context
createTargetFromContext :: Context -> IO PTX

-- | Configure the default execution target to allocate all future
--   host-side arrays using (CUDA) pinned memory. Any newly allocated
--   arrays will be page-locked and directly accessible from the device,
--   enabling high-speed (asynchronous) DMA.
--   
--   Note that since the amount of available pageable memory will be
--   reduced, overall system performance can suffer.
registerPinnedAllocator :: IO ()

-- | As with <a>registerPinnedAllocator</a>, but configure the given
--   execution context.
registerPinnedAllocatorWith :: PTX -> IO ()
instance Data.Array.Accelerate.LLVM.PTX.RunAsync b => Data.Array.Accelerate.LLVM.PTX.RunAsync (a -> b)
instance Data.Array.Accelerate.LLVM.PTX.RunAsync (GHC.Types.IO (Data.Array.Accelerate.Async.Async b))
